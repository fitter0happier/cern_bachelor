{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6395d1fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "\n",
    "\n",
    "from lib import *\n",
    "\n",
    "\n",
    "t = CEventsTable()\n",
    "t.appendFromCsv(\"ivan_ntuples/tH.csv\",1,\"tH\")\n",
    "t.appendFromCsv(\"ivan_ntuples/ttb.csv\",0,\"ttb\")\n",
    "t.appendFromCsv(\"ivan_ntuples/ttc.csv\",0,\"ttc\")\n",
    "t.appendFromCsv(\"ivan_ntuples/ttL.csv\",0,\"ttL\")\n",
    "t.appendFromCsv(\"nazim_ntuples/ttH.csv\",0,\"ttH\")\n",
    "t.appendFromCsv(\"nazim_ntuples/ttZ.csv\",0,\"ttZ\")\n",
    "t.appendFromCsv(\"ivan_ntuples/ttW.csv\",0,\"ttW\")\n",
    "t.appendFromCsv(\"ivan_ntuples/tZq.csv\",0,\"tZq\")\n",
    "t.appendFromCsv(\"ivan_ntuples/tWZ.csv\",0,\"tWZ\")\n",
    "t.appendFromCsv(\"ivan_ntuples/tW.csv\",0,\"single_tW\")\n",
    "t.appendFromCsv(\"ivan_ntuples/single_t1.csv\",0,\"single_tt\")\n",
    "t.appendFromCsv(\"ivan_ntuples/single_t2.csv\",0,\"single_tt\")\n",
    "t.appendFromCsv(\"ivan_ntuples/single_ts.csv\",0,\"single_ts\")\n",
    "t.appendFromCsv(\"nazim_ntuples/WZ.csv\",0,\"WZ\")\n",
    "t.appendFromCsv(\"nazim_ntuples/VV1.csv\",0,\"VV\")\n",
    "t.appendFromCsv(\"nazim_ntuples/VV2.csv\",0,\"VV\")\n",
    "t.appendFromCsv(\"ivan_ntuples/data.csv\",0,\"nonp\")\n",
    "\n",
    "\n",
    "\n",
    "print('Before preselection:')\n",
    "t.printTypesNumbers()\n",
    "t.applyPreselection()\n",
    "print('After preselection:')\n",
    "t.printTypesNumbers()\n",
    "print('With weights:')\n",
    "t.calculateWeights()\n",
    "t.printTypesNumbers()\n",
    "    \n",
    "\n",
    "\n",
    "#mld = CCreateDfRatioAll(t.table, 0.7, create_val = False)\n",
    "np.set_printoptions(suppress=True)\n",
    "pd.options.display.float_format = '{:.8f}'.format\n",
    "\n",
    "\n",
    "\n",
    "mld = CSampledDatasetExpEvents(t.table, (20000,100000), (20000,100000))\n",
    "#mld = CSampledDatasetAcc(t.table, (20000,100000), (10000,100000))\n",
    "mld.createStandartizedDatasets()\n",
    "\n",
    "mld.printInfo()\n",
    "\n",
    "\n",
    "X_train = mld.X_train_s\n",
    "y_train = mld.y_train\n",
    "types_train = mld.types_train\n",
    "weights_train = mld.weights_train\n",
    "\n",
    "X_test = mld.X_val_s\n",
    "y_test = mld.y_val\n",
    "types_test = mld.types_val\n",
    "weights_test = mld.weights_val\n",
    "\n",
    "\n",
    "\n",
    "AucCalc = CEvaluationAUC()\n",
    "SigCalc = CEvaluationSignificance()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb07df27",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class CObjective:\n",
    "    def __init__(self, filepath):\n",
    "        self.filepath = filepath\n",
    "        self.df = pd.DataFrame()\n",
    "        self.df.to_csv(self.filepath, index=False)\n",
    "        self.model = None\n",
    "\n",
    "\n",
    "    def __call__(self, trial):\n",
    "        start = time.time()\n",
    "        tf.random.set_seed(10)\n",
    "\n",
    "\n",
    "        model = Sequential()\n",
    "\n",
    "        init = tf.keras.initializers.GlorotNormal(seed=10)\n",
    "\n",
    "        n_layers = trial.suggest_int('n_layers',1,10)\n",
    "        \n",
    "        layer_info = []\n",
    "        for i in range(n_layers):    \n",
    "            activation = trial.suggest_categorical(\"activation_l{}\".format(i),['tanh','relu','sigmoid'])\n",
    "            n_units = trial.suggest_int(\"n_units_l{}\".format(i),1,150)\n",
    "\n",
    "            model.add(Dense(n_units, kernel_initializer=init, activation=activation))\n",
    "\n",
    "            dropout = trial.suggest_float(\"dropout_l{}\".format(i),0.0, 0.5)\n",
    "            model.add(Dropout(dropout))\n",
    "            \n",
    "            layer_info.append([activation, n_units, dropout])\n",
    "\n",
    "\n",
    "        model.add(Dense(1, kernel_initializer=init, activation = 'sigmoid'))\n",
    "\n",
    "        lr = trial.suggest_float(\"lr\", 1e-5, 1e-1)\n",
    "\n",
    "        #optimizer = trial.suggest_categorical(\"optimizer\",['adam','sgd'])\n",
    "        optimizer = Adam(learning_rate=lr)\n",
    "\n",
    "        loss_f = trial.suggest_categorical(\"loss_f\",['mse','binary_crossentropy','binary_focal_crossentropy'])\n",
    "\n",
    "        model.compile(loss=loss_f, optimizer=optimizer, metrics=['AUC'])\n",
    "        \n",
    "        callback_loss = EarlyStopping(monitor='loss', patience=4)\n",
    "        callback_auc = EarlyStopping(monitor='auc', patience=4)\n",
    "        history = model.fit(X_train, y_train, epochs = 50, batch_size = 100, \n",
    "                            callbacks=[callback_auc, callback_loss], verbose = 1 )\n",
    "        self.model = model\n",
    "        \n",
    "        #calculate accuracy\n",
    "        preds_train = model.predict(X_train)[:,0]\n",
    "        preds_test = model.predict(X_test)[:,0]\n",
    "    \n",
    "        auc_train = AucCalc.evaluate(preds_train, y_train)\n",
    "        auc_test = AucCalc.evaluate(preds_test, y_test)\n",
    "        significance, thr, sig, bg = SigCalc.evaluate(preds_test, y_test, weights_test)\n",
    "\n",
    "        end = time.time()\n",
    "        elapsed_time = end - start\n",
    "        \n",
    "        n_epochs = len(history.history['loss'])\n",
    "        \n",
    "        print('n_epochs:', n_epochs)\n",
    "        print('auc train:',auc_train)\n",
    "        print('auc test:',auc_test)\n",
    "        print('best significance:', significance)\n",
    "        print('time:',elapsed_time)\n",
    "        \n",
    "        line = pd.DataFrame({\n",
    "            'n_layers':[n_layers], 'lr' : [lr], 'loss_f' : [loss_f],           \n",
    "            'n_epochs':[n_epochs], 'auc_train':[auc_train], 'auc_test': [auc_test],\n",
    "            'sig_test':[significance], 'thr':[thr], 'n_sig': [sig], 'n_bg':[bg],\n",
    "            'time':[elapsed_time], 'n_epochs' : [n_epochs]})\n",
    "            \n",
    "        for i in range(len(layer_info)):\n",
    "            line['l'+str(i)+'_activation'] = layer_info[i][0] \n",
    "            line['l'+str(i)+'_nunits'] = layer_info[i][1] \n",
    "            line['l'+str(i)+'_dropout'] = layer_info[i][2] \n",
    "                        \n",
    "            \n",
    "        self.df = pd.concat([self.df, line])\n",
    "        self.df.to_csv(self.filepath, index=False)\n",
    "            \n",
    "\n",
    "        return auc_test\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "for i in list(range(0,20)):    \n",
    "    if i == 0:\n",
    "        sampler = optuna.samplers.NSGAIISampler(seed=11)\n",
    "        #sampler = optuna.samplers.TPESampler(consider_prior = True, seed=11 )\n",
    "        study = optuna.create_study(direction='maximize', sampler = sampler)\n",
    "    else:\n",
    "        study = joblib.load(study_filename)\n",
    "    \n",
    "    study_filename = \"nn_nsga/study\" + str(i)\n",
    "    df_filename = \"nn_nsga/df\" + str(i)\n",
    "    \n",
    "    obj = CObjective(df_filename)\n",
    "    study.optimize(obj, n_trials=100)    \n",
    "\n",
    "    joblib.dump(study, study_filename)\n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

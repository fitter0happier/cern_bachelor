{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5483a97e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lib import *\n",
    "\n",
    "from xgboost.callback import TrainingCallback\n",
    "from xgboost.callback import LearningRateScheduler\n",
    "\n",
    "\n",
    "class myTrainingMonitor(xgb.callback.TrainingCallback):\n",
    "    def __init__(self, xgbmodel, test_used = False):\n",
    "        self.xgbmodel = xgbmodel\n",
    "        self.test_used = test_used\n",
    "        pass\n",
    "    \n",
    "    def after_iteration(self, model, epoch, evals_log):\n",
    "        if self.test_used:\n",
    "            print(epoch, 'auc train:', evals_log['eval_train']['auc'][-1], \n",
    "                  'auc test:', evals_log['eval_test']['auc'][-1])\n",
    "        else:\n",
    "            print(epoch, 'auc train:', evals_log['eval_train']['auc'][-1])            \n",
    "\n",
    "\n",
    "\n",
    "class myLRScheduler(xgb.callback.TrainingCallback):\n",
    "    def __init__(self, xgbmodel, learning_rate):\n",
    "        #super().__init__(lr_scheduler)\n",
    "        self.xgbmodel = xgbmodel\n",
    "\n",
    "        self.last_update = 0\n",
    "        self.minimum_improvement = 0.00000010\n",
    "        self.lr = learning_rate\n",
    "        \n",
    "        # for thr_improvement func\n",
    "        self.min_impr_for_lr = 0.0001\n",
    "        \n",
    "        self.best_model = None\n",
    "        self.best_performance = 0\n",
    "        self.best_epoch = -1\n",
    "        self.n_epochs = 0\n",
    "             \n",
    "    \n",
    "    def giveLr(self, epoch):        \n",
    "        #print('lr:',self.lr)\n",
    "        return self.lr\n",
    "    \n",
    "    \n",
    "    def after_iteration(self, model, epoch, evals_log):\n",
    "        if evals_log['eval_test']['auc'][-1] > self.best_performance:\n",
    "            self.best_performance = evals_log['eval_test']['auc'][-1]\n",
    "            self.best_performance_train = evals_log['eval_train']['auc'][-1]\n",
    "            self.best_model = model.copy()\n",
    "            self.best_epoch = epoch\n",
    "            \n",
    "        #self.thr_improvement(model, epoch, evals_log)\n",
    "        return self.no_impr_rounds(model, epoch, evals_log)\n",
    "            \n",
    "    \n",
    "    def no_impr_rounds(self, model, epoch, evals_log):\n",
    "        self.n_epochs = epoch\n",
    "        \n",
    "        if (epoch - self.last_update) > 5:\n",
    "            no_impr = True\n",
    "             \n",
    "            for i in range(-5,0):\n",
    "                if  evals_log['eval_test']['auc'][-6] < evals_log['eval_test']['auc'][i]:\n",
    "                    no_impr = False\n",
    "                    break\n",
    "            \n",
    "            if no_impr:\n",
    "                if self.lr < 0.0001:\n",
    "                    return True\n",
    "                \n",
    "                self.lr /= 1.3\n",
    "                self.last_update = epoch\n",
    "                #print('no impr')\n",
    "                #print('new lr:', self.lr)\n",
    "                \n",
    "    \n",
    "    def thr_improvement(self, model, epoch, evals_log):        \n",
    "        if (epoch - self.last_update) > 5:  \n",
    "            minus_one = evals_log['eval_test']['auc'][-1]\n",
    "            minus_five = evals_log['eval_test']['auc'][-5]\n",
    "            diff = evals_log['eval_test']['auc'][-1] - evals_log['eval_test']['auc'][-5]\n",
    "            print('diff:',diff, minus_one - minus_five)\n",
    "            print('-1:',evals_log['eval_test']['auc'][-1])\n",
    "            print('-5:',evals_log['eval_test']['auc'][-5])\n",
    "            \n",
    "            if diff < self.minimum_improvement and self.lr <= 0.00001:\n",
    "                return True\n",
    "\n",
    "            if diff < self.min_impr_for_lr:\n",
    "                self.last_update = epoch\n",
    "                self.lr /= 10\n",
    "                self.min_impr_for_lr /= 10 \n",
    "                \n",
    "                print('----NEW LEARNING RATE APPLIED---------')\n",
    "                print(self.lr)\n",
    "                \n",
    "                \n",
    "    def after_training(self, model):\n",
    "        #print('best auc:', self.best_performance)\n",
    "        #print('best epoch:', self.best_epoch)\n",
    "        \n",
    "        return self.best_model\n",
    "        \n",
    "        \n",
    "        \n",
    "class XGBWrapper:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def train(self, train_m, n_estimators, max_depth, learning_rate, eval_m = None, \n",
    "              booster = 'gbtree',  scale_pos_weight = 1.0, lambdaa = 1.0, alpha = 0, gamma = 0,\n",
    "              min_child_weight = 1, colsample_bytree = 1.0):\n",
    "        \n",
    "        params = {}\n",
    "        params['booster'] = booster\n",
    "        params['scale_pos_weight'] = scale_pos_weight\n",
    "        params['lambda'] = lambdaa\n",
    "        params['alpha'] = alpha\n",
    "        params['gamma'] = gamma\n",
    "        params['objective'] = 'binary:logistic'\n",
    "        params['eval_metric'] = 'auc'\n",
    "        params['booster'] = 'gbtree'\n",
    "        params['max_depth'] = max_depth\n",
    "        params['min_child_weight'] = min_child_weight\n",
    "        params['colsample_bytree'] = colsample_bytree\n",
    "        params['seed'] = 10\n",
    "        #params['eta'] = learning_rate\n",
    "        #params['tree_method'] = 'exact'\n",
    "        \n",
    "        if eval_m == None:\n",
    "            evallist = [(train_m, 'eval_train')]\n",
    "            trainingCallback = myTrainingMonitor(self, False)\n",
    "        else:\n",
    "            evallist = [(train_m, 'eval_train'),(eval_m, 'eval_test')]\n",
    "            trainingCallback = myTrainingMonitor(self, True)\n",
    "            \n",
    "        lrClass = myLRScheduler(self, learning_rate)\n",
    "        lrCallback = xgb.callback.LearningRateScheduler(lrClass.giveLr)\n",
    "        \n",
    "        num_round = n_estimators        \n",
    "        \n",
    "        start = time.time()\n",
    "        model = xgb.train(params,train_m,num_round,  evals = evallist, \n",
    "                          callbacks=[lrClass, lrCallback], \n",
    "                          early_stopping_rounds = 500, verbose_eval = False)\n",
    "        \n",
    "        model = lrClass.best_model\n",
    "        \n",
    "        self.best_performance = lrClass.best_performance\n",
    "        self.best_performance_train = lrClass.best_performance_train\n",
    "        self.best_epoch = lrClass.best_epoch\n",
    "        self.n_epochs = lrClass.n_epochs\n",
    "                \n",
    "        \n",
    "        self.model = model\n",
    "        end = time.time()\n",
    "        self.elapsed_time = end - start\n",
    "        \n",
    "    def predict(self, X_test):\n",
    "        p = xgb.DMatrix(X_test)\n",
    "        return self.model.predict(p)\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "t = CEventsTable()\n",
    "t.appendFromCsv(\"ivan_ntuples/tH.csv\",1,\"tH\")\n",
    "t.appendFromCsv(\"ivan_ntuples/ttb.csv\",0,\"ttb\")\n",
    "t.appendFromCsv(\"ivan_ntuples/ttc.csv\",0,\"ttc\")\n",
    "t.appendFromCsv(\"ivan_ntuples/ttL.csv\",0,\"ttL\")\n",
    "t.appendFromCsv(\"nazim_ntuples/ttH.csv\",0,\"ttH\")\n",
    "t.appendFromCsv(\"nazim_ntuples/ttZ.csv\",0,\"ttZ\")\n",
    "t.appendFromCsv(\"ivan_ntuples/ttW.csv\",0,\"ttW\")\n",
    "t.appendFromCsv(\"ivan_ntuples/tZq.csv\",0,\"tZq\")\n",
    "t.appendFromCsv(\"ivan_ntuples/tWZ.csv\",0,\"tWZ\")\n",
    "t.appendFromCsv(\"ivan_ntuples/tW.csv\",0,\"single_tW\")\n",
    "t.appendFromCsv(\"ivan_ntuples/single_t1.csv\",0,\"single_tt\")\n",
    "t.appendFromCsv(\"ivan_ntuples/single_t2.csv\",0,\"single_tt\")\n",
    "t.appendFromCsv(\"ivan_ntuples/single_ts.csv\",0,\"single_ts\")\n",
    "t.appendFromCsv(\"nazim_ntuples/WZ.csv\",0,\"WZ\")\n",
    "t.appendFromCsv(\"nazim_ntuples/VV1.csv\",0,\"VV\")\n",
    "t.appendFromCsv(\"nazim_ntuples/VV2.csv\",0,\"VV\")\n",
    "t.appendFromCsv(\"ivan_ntuples/data.csv\",0,\"nonp\")\n",
    "\n",
    "t.subSample('ttH', 0.1)\n",
    "\n",
    "\n",
    "\n",
    "print('Before preselection:')\n",
    "t.printTypesNumbers()\n",
    "t.applyPreselection()\n",
    "print('After preselection:')\n",
    "t.printTypesNumbers()\n",
    "print('With weights:')\n",
    "t.calculateWeights()\n",
    "t.printTypesNumbers()    \n",
    "    \n",
    "    \n",
    "mld = CSampledDatasetExpEvents(t.table, (20000,100000), (10000,100000))\n",
    "#mld = CSampledDatasetAcc(t.table, (20000,100000), (10000,100000))\n",
    "mld.printInfo()\n",
    "\n",
    "\n",
    "X_train = mld.X_train\n",
    "y_train = mld.y_train\n",
    "types_train = mld.types_train\n",
    "weights_train = mld.weights_train\n",
    "\n",
    "X_test = mld.X_test\n",
    "y_test = mld.y_test\n",
    "types_test = mld.types_test\n",
    "weights_test = mld.weights_test\n",
    "\n",
    "\n",
    "X_val = mld.X_val\n",
    "y_val = mld.y_val\n",
    "types_val = mld.types_val\n",
    "weights_val = mld.weights_val\n",
    "\n",
    "\n",
    "dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "dtest = xgb.DMatrix(X_test, label=y_test)\n",
    "dval = xgb.DMatrix(X_val, label=y_val)\n",
    "\n",
    "\n",
    "\n",
    "AucCalc = CEvaluationAUC()\n",
    "SigCalc = CEvaluationSignificance()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5803e29",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CObjective:\n",
    "    def __init__(self, filepath):\n",
    "        self.filepath = filepath\n",
    "        self.df = pd.DataFrame()\n",
    "        self.df.to_csv(self.filepath, index=False)\n",
    "\n",
    "    def __call__(self, trial):\n",
    "            \n",
    "        # booster, def gbtree\n",
    "        #booster = trial.suggest_categorical('booster', ['gbtree', 'gblinear', 'dart'])\n",
    "        booster = 'gbtree'\n",
    "\n",
    "        # maximalni hloubka, def 6\n",
    "        #max_depth = trial.suggest_int('max_depth', 3, 7)\n",
    "        max_depth = 5\n",
    "\n",
    "        # vaha kladne tridy, def 1\n",
    "        #scale_pos_weight =  trial.suggest_float('scale_pos_weight', 1.0, 5.0)\n",
    "        scale_pos_weight = 1.0\n",
    "\n",
    "        # L2 regularizace pro vahy, def 1\n",
    "        lambdaa =  trial.suggest_float('lambda', 0, 15.0)\n",
    "\n",
    "        # L1 regularizace pro vahy, def 0\n",
    "        # poznamka - rozsiren rozsah, protoze predchozi hodnoty byly blizko 10\n",
    "        alpha =  trial.suggest_float('alpha', 0, 50.0)\n",
    "\n",
    "        # Minimum loss reduction required to make a further partition on a leaf node of the tree, def 0\n",
    "        gamma =  trial.suggest_float('gamma', 0, 8.0)\n",
    "\n",
    "\n",
    "        # min_child_weight - Minimum sum of instance weight (hessian) needed in a child.\n",
    "        # If the tree partition step results in a leaf node with the sum of instance weight less than min_child_weight, \n",
    "        # then the building process will give up further partitioning. def 1\n",
    "        min_child_weight = trial.suggest_float('min_child_weight', 0, 20.0)\n",
    "\n",
    "        # colsample_bytree is the subsample ratio of columns when constructing each tree. def 1\n",
    "        colsample_bytree = 0.6 #trial.suggest_float('colsample_bytree', 0.5, 1.0)\n",
    "\n",
    "\n",
    "        model = XGBWrapper()\n",
    "        model.train(dtrain, 10000, max_depth, 0.1, dval, \n",
    "                    scale_pos_weight = scale_pos_weight, lambdaa = lambdaa,\n",
    "                    alpha = alpha, gamma = gamma, booster = booster, \n",
    "                    min_child_weight = min_child_weight, colsample_bytree = colsample_bytree)\n",
    "\n",
    "        best_performance = model.best_performance\n",
    "        best_performance_train = model.best_performance_train\n",
    "        best_epoch = model.best_epoch\n",
    "        n_epochs = model.n_epochs\n",
    "        el_time =  model.elapsed_time\n",
    "\n",
    "        print('\\ntraining:')\n",
    "        print('best auc:',best_performance,'best epoch:', best_epoch, 'epoch cnt:', n_epochs, \n",
    "              'time:',el_time, 'best train auc:', best_performance_train)\n",
    "\n",
    "        preds = model.predict(X_test)\n",
    "\n",
    "        auc = AucCalc.evaluate(preds, y_test)\n",
    "\n",
    "        significance, thr, sig, bg = SigCalc.evaluate(preds, y_test, weights_test)\n",
    "\n",
    "        print('test auc:', auc)\n",
    "        print('test sig:', significance)\n",
    "\n",
    "        line = pd.DataFrame({  'min_child_weight' : [min_child_weight], 'colsample_bytree' : [colsample_bytree],\n",
    "                  'lambda': [lambdaa], 'alpha':[alpha], 'gamma' : [gamma], 'best_auc_val':[best_performance], 'best_auc_train':[best_performance_train],\n",
    "                  'auc_test':[auc], 'best_significance':[significance], 'n_sig':[sig],'n_bg':[bg],'thr':[thr], \n",
    "                  'n_epochs':[n_epochs], 'best_epoch': [best_epoch], 'time':[el_time]})\n",
    "\n",
    "        self.df = pd.concat([self.df, line])\n",
    "        self.df.to_csv(self.filepath, index=False)\n",
    "\n",
    "        return significance\n",
    "\n",
    "\n",
    "\n",
    "for i in range(4):    \n",
    "    if i == 0:\n",
    "        #sampler = optuna.samplers.NSGAIISampler(seed=11)\n",
    "        sampler = optuna.samplers.TPESampler(consider_prior = True, seed=11 )\n",
    "        study = optuna.create_study(direction='maximize', sampler = sampler)\n",
    "    else:\n",
    "        study = joblib.load(study_filename)\n",
    "    \n",
    "    study_filename = \"tpe_sampler_sig/study\" + str(i)\n",
    "    df_filename = \"tpe_sampler_sig/df\" + str(i)\n",
    "    \n",
    "    \n",
    "    study.optimize(CObjective(df_filename), n_trials=500)    \n",
    "\n",
    "    joblib.dump(study, study_filename)\n",
    "\n",
    "    \n",
    "print(optuna.importance.get_param_importances(study))\n",
    "print(optuna.importance.MeanDecreaseImpurityImportanceEvaluator().evaluate(study))\n",
    "print(optuna.importance.FanovaImportanceEvaluator().evaluate(study))\n",
    "\n",
    "study.trials_dataframe().to_csv('tpe_sampler_sig/trials_dataframe.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cern_bachelor",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
